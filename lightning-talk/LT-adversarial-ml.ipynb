{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Convert to HTML file via `jupyter nbconvert LT-adversarial-ml.ipynb --to slides`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adversarial Machine Learning\n",
    "\n",
    "How to attack a deep learning model\n",
    "\n",
    "*Alexander Engelhardt*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General idea\n",
    "\n",
    "- Change your data point *slightly*\n",
    "  - a human doesn't notice the change\n",
    "  - the model predicts something entirely different\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adversarial Patch\n",
    "\n",
    "![adversarial patch](img/adversarial-patch.png)\n",
    "\n",
    "Brown, Tom B., et al. \"Adversarial patch.\" arXiv preprint arXiv:1712.09665 (2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Now, imagine placing that sticker on a stop sign. Then it gets serious, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adversarial Turtle\n",
    "\n",
    "![adversarial turtle](img/adversarial-turtle.png)\n",
    "\n",
    "Athalye, Anish, et al. \"Synthesizing robust adversarial examples.\" arXiv preprint arXiv:1707.07397 (2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- This is a 3d printed turtle, that's misclassified as a rifle from *many* angles and zoom levels\n",
    "- Now, imagine creating a rifle that gets classified as a harmless turtle. Then it gets serious, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adversarial Audio\n",
    "\n",
    "<img src=\"img/adversarial-audio.png\" style=\"height: 500px;\"/>\n",
    "\n",
    "Carlini, Nicholas, and David Wagner. \"Audio adversarial examples: Targeted attacks on speech-to-text.\" 2018 IEEE Security and Privacy Workshops (SPW). IEEE, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Now, imagine airing a TV commercial with a manipulated audio that gets transcribed to \"Alexa, play Despacito at full volume\". Then it gets serious, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to attack a model?\n",
    "\n",
    "- You don't need access to the model itself, only to its prediction API (e.g. Google's image classifier)!\n",
    "\n",
    "----\n",
    "\n",
    "1. Create your own training data set\n",
    "2. Get the predictions from Google's black-box model\n",
    "3. Train your own surrogate model with the predictions as a target\n",
    "    - You want to approximate the original model's decision boundaries\n",
    "4. Use *your* model to create adversarial images **for your model**\n",
    "    - e.g. Ian Goodfellow's \"Fast Gradient Sign Method\"\n",
    "5. The **original black-box model** also gets fooled by these adversarial images!\n",
    "\n",
    "----\n",
    "\n",
    "Papernot, Nicolas, et al. \"Practical black-box attacks against machine learning.\" Proceedings of the 2017 ACM on Asia conference on computer and communications security. ACM, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to defend against them?\n",
    "\n",
    "- New research field: Interpretable Machine Learning\n",
    "  - My talk, Friday 2pm\n",
    "- Adversarial training\n",
    "  - Talk by Irina Vidal Migall√≥n, Thursday 12:25pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thanks!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
